import re
import sys
import random
import math
import collections


class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a language model
        from a given text.
        It supports language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False.
        """
        self.n = n
        self.model_dict = collections.defaultdict(
            int)  # a dictionary of the form {ngram:count}, holding counts of all ngrams in the specified text.
        self.chars = chars
        self.unigram_dict = collections.defaultdict(int)
        self.split_by = "" if self.chars else " "

    def split_to_unigrams(self, text):
        """
        Splits a text into a list of tokens
            Args:
                text (str): the text to split.
            Return:
                (list): tokens, splitted by "" if the model consists of ngrams of characters rather then word tokens,
                else by " ".
        """
        return list(text) if self.chars else text.split(" ")

    def split_to_n_grams(self, text):
        """
        Splits the input text into n-grams
            Args:
                text (str): the text to split.
            Return:
                (list): n-grams list
        """
        n_grams = []
        splitted_text = self.split_to_unigrams(text)
        if len(splitted_text) < self.n:
            return splitted_text
        for i in range(len(splitted_text) - (self.n - 1)):
            n_gram = self.split_by.join(splitted_text[i:i + self.n])
            n_grams.append(n_gram)
        return n_grams

    def build_model(self, text):
        """populates the instance variable model_dict.

            Args:
                text (str): the text to construct the model from.
        """
        n_grams_list = self.split_to_n_grams(text)
        unigrams = self.split_to_unigrams(text)

        # build the n-gram dictionary
        for n_gram in n_grams_list:
            occur_num = self.model_dict.setdefault(n_gram, 0)
            self.model_dict.update({n_gram: occur_num + 1})

        # build the unigram dictionary
        for unigram in unigrams:
            occur_num = self.unigram_dict.setdefault(unigram, 0)
            self.unigram_dict.update({unigram: occur_num + 1})

    def get_model_dictionary(self):
        """Returns the dictionary class object
        """
        return self.model_dict

    def get_model_window_size(self):
        """Returning the size of the context window (the n in "n-gram")
        """
        return self.n

    def get_markov_n_minus_dict(self, ngram):
        """
        finds all the ngram's matching keys in the model
            Args:
                ngram (str): the ngram to find
            Return:
                (defaultdict): all options found in the model

        """
        markov_options = {}
        for key in self.model_dict.keys():
            if key.startswith(ngram):
                mo_key = key if key == "" else self.split_to_unigrams(key)[self.n - 1]
                markov_options[mo_key] = self.model_dict.get(key)
        return markov_options

    def sample_context(self):
        """
        Samples a new context from the model's distribution
            Return:
                (str): a sampled context
        """
        return random.choices(list(self.model_dict.keys()), weights=list(self.model_dict.values()))[0]

    def is_exhausted_context(self, context):
        """
        Checks if the context is exhausted or not
            Args:
                (str): the context
            Return:
                (bool): True if the context is exhausted, False otherwise.
        """
        return len(self.get_markov_n_minus_dict(context)) == 0

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context.

        If the length of the specified context exceeds (or equal to) the specified n, the method returns the prefix
        of length n of the specified context.

        A new context is sampled from the models' contexts distribution in the following cases:
            * The context is shorter than n-1. The context will be added to the output String
            * No context is specified.

        Generation stops before the n'th word for the following cases:
            * The contexts are exhausted.
            * Preliminary context is not in the vocabulary.
            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.
        """
        chain = []

        # If no context is specified the context should be sampled from the models' contexts distribution.
        if context is None:
            context = self.sample_context()

        # normalize the context
        context = normalize_text(context)

        # context is shorter than n-1
        if len(self.split_to_unigrams(context)) < self.n - 1:
            chain.append(context)
            if self.is_exhausted_context(context):
                return self.split_by.join(chain)
            else:
                context = self.sample_context()

        # concat context to the output word chain
        chain.extend(self.split_to_unigrams(context))
        chain_len = len(chain)

        # context is longer then the output text length
        if chain_len >= n:
            return self.split_by.join(chain)

        while True:
            context = self.split_by.join(chain[-self.n + 1:])
            if self.is_exhausted_context(context):
                break

            # If generated text reached the n'th word
            if len(chain) == n:
                break

            else:
                markov_options = self.get_markov_n_minus_dict(context)
                chosen_key = random.choices(list(markov_options.keys()), weights=list(markov_options.values()))[0]
                chain.append(chosen_key)
        return self.split_by.join(chain)

    def count_occure(self, word):
        """
        Returns the number of word occurrences in the model
            Args:
                (str) ngram or a part of it
            Return:
                (int) num of occurrences
        """
        counter = 0
        for key in self.model_dict.keys():
            if key.startswith(word):
                counter += self.model_dict.get(key, 0)
        return counter

    def evaluate(self, text):
        """Returns the log-likelihood of the specified text to be a product of the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        probs = []
        n_grams_list = self.split_to_n_grams(text)
        unigrams_size = sum(self.unigram_dict.values())  # number of unigrams in total
        ngram_size = sum(self.model_dict.values())

        first_ngram = n_grams_list[0]
        first_unigrams = [self.split_by.join(self.split_to_unigrams(first_ngram)[0:i]) for i in range(1, self.n)]

        for i, ngram in enumerate(first_unigrams):
            sum_occurences = 0
            ngram_prob = 0
            _ngram = ""

            _occur = self.count_occure(ngram)
            ngram_split = ngram.split(self.split_by)
            denominator = ngram_size if len(ngram_split) == 1 else self.count_occure(self.split_by.join(ngram_split[:i]))
            _prob = _occur / denominator
            log_prob = math.log(_prob)
            probs.append(log_prob)

        for ngram in n_grams_list:
            n_gram_count = self.model_dict.get(ngram)
            partition = ngram[:self.n - 1] if self.chars else ngram.rpartition(self.split_by)[0]
            n_minus_count = sum(self.get_markov_n_minus_dict(partition).values())
            ngram_prob = self.smooth(ngram) if n_gram_count is None else n_gram_count / n_minus_count
            log_prob = math.log(ngram_prob)
            probs.append(log_prob)
        return sum(probs)

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.
            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """

        n_grams_size = sum(self.model_dict.values())  # total num of tokens in the dictionary
        v = sum(self.unigram_dict.values())  # all words in corpus
        return 1 / (n_grams_size + v)


def normalize_text(text):
    """Returns a normalized version of the specified string.
        Performs the following operations:
        * Lower casing
        * Padding punctuation with white spaces
        * Expanding contractions - for example: ain't --> am not

      Args:
        text (str): the text to normalize

      Returns:
        string. the normalized text.
    """

    def expand_contractions(s):
        contractions_dict = {
            "ain't": "am not",
            "aren't": "are not",
            "can't": "cannot",
            "can't've": "cannot have",
            "'cause": "because",
            "could've": "could have",
            "couldn't": "could not",
            "couldn't've": "could not have",
            "didn't": "did not",
            "doesn't": "does not",
            "don't": "do not",
            "hadn't": "had not",
            "hadn't've": "had not have",
            "hasn't": "has not",
            "haven't": "have not",
            "he'd": "he would",
            "he'd've": "he would have",
            "he'll": "he will",
            "he'll've": "he will have",
            "he's": "he is",
            "how'd": "how did",
            "how'd'y": "how do you",
            "how'll": "how will",
            "how's": "how is",
            "I'd": "I would",
            "I'd've": "I would have",
            "I'll": "I will",
            "I'll've": "I will have",
            "I'm": "I am",
            "I've": "I have",
            "isn't": "is not",
            "it'd": "it had",
            "it'd've": "it would have",
            "it'll": "it will",
            "it'll've": "it will have",
            "it's": "it is",
            "let's": "let us",
            "ma'am": "madam",
            "mayn't": "may not",
            "might've": "might have",
            "mightn't": "might not",
            "mightn't've": "might not have",
            "must've": "must have",
            "mustn't": "must not",
            "mustn't've": "must not have",
            "needn't": "need not",
            "needn't've": "need not have",
            "o'clock": "of the clock",
            "oughtn't": "ought not",
            "oughtn't've": "ought not have",
            "shan't": "shall not",
            "sha'n't": "shall not",
            "shan't've": "shall not have",
            "she'd": "she would",
            "she'd've": "she would have",
            "she'll": "she will",
            "she'll've": "she will have",
            "she's": "she is",
            "should've": "should have",
            "shouldn't": "should not",
            "shouldn't've": "should not have",
            "so've": "so have",
            "so's": "so is",
            "that'd": "that would",
            "that'd've": "that would have",
            "that's": "that is",
            "there'd": "there had",
            "there'd've": "there would have",
            "there's": "there is",
            "they'd": "they would",
            "they'd've": "they would have",
            "they'll": "they will",
            "they'll've": "they will have",
            "they're": "they are",
            "they've": "they have",
            "to've": "to have",
            "wasn't": "was not",
            "we'd": "we had",
            "we'd've": "we would have",
            "we'll": "we will",
            "we'll've": "we will have",
            "we're": "we are",
            "we've": "we have",
            "weren't": "were not",
            "what'll": "what will",
            "what'll've": "what will have",
            "what're": "what are",
            "what's": "what is",
            "what've": "what have",
            "when's": "when is",
            "when've": "when have",
            "where'd": "where did",
            "where's": "where is",
            "where've": "where have",
            "who'll": "who will",
            "who'll've": "who will have",
            "who's": "who is",
            "who've": "who have",
            "why's": "why is",
            "why've": "why have",
            "will've": "will have",
            "won't": "will not",
            "won't've": "will not have",
            "would've": "would have",
            "wouldn't": "would not",
            "wouldn't've": "would not have",
            "y'all": "you all",
            "y'alls": "you alls",
            "y'all'd": "you all would",
            "y'all'd've": "you all would have",
            "y'all're": "you all are",
            "y'all've": "you all have",
            "you'd": "you had",
            "you'd've": "you would have",
            "you'll": "you you will",
            "you'll've": "you you will have",
            "you're": "you are",
            "you've": "you have"
        }
        contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))

        def replace(match):
            return contractions_dict[match.group(0)]

        return contractions_re.sub(replace, s)

    characters_to_pad = r"([\w/'+$\s-]+|[^\w/'+$\s-]+)\s*"

    # case conversion
    lower_case_txt = text.lower()

    # extend contractions
    extended_txt = expand_contractions(lower_case_txt)

    # pad punctuation
    padded_txt = re.sub(characters_to_pad, r"\1 ", extended_txt)
    trimmed_padded = padded_txt.rstrip()

    return trimmed_padded


def who_am_i():  # this is not a class method
    """Returns a dictionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'Amit Shakarchy', 'id': '313278889', 'email': 'shakarch@post.bgu.ac.il'}
